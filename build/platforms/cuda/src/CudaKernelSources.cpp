/* -------------------------------------------------------------------------- *
 *                                   OpenMM                                   *
 * -------------------------------------------------------------------------- *
 * This is part of the OpenMM molecular simulation toolkit originating from   *
 * Simbios, the NIH National Center for Physics-Based Simulation of           *
 * Biological Structures at Stanford, funded under the NIH Roadmap for        *
 * Medical Research, grant U54 GM072970. See https://simtk.org.               *
 *                                                                            *
 * Portions copyright (c) 2012 Stanford University and the Authors.           *
 * Authors: Peter Eastman                                                     *
 * Contributors:                                                              *
 *                                                                            *
 * This program is free software: you can redistribute it and/or modify       *
 * it under the terms of the GNU Lesser General Public License as published   *
 * by the Free Software Foundation, either version 3 of the License, or       *
 * (at your option) any later version.                                        *
 *                                                                            *
 * This program is distributed in the hope that it will be useful,            *
 * but WITHOUT ANY WARRANTY; without even the implied warranty of             *
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              *
 * GNU Lesser General Public License for more details.                        *
 *                                                                            *
 * You should have received a copy of the GNU Lesser General Public License   *
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.      *
 * -------------------------------------------------------------------------- */

#include "CudaKernelSources.h"

using namespace OpenMM;
using namespace std;

const string CudaKernelSources::common = "/**\n"
" * This file contains CUDA definitions for the macros and functions needed for the\n"
" * common compute framework.\n"
" */\n"
"\n"
"#define KERNEL extern \"C\" __global__\n"
"#define DEVICE __device__\n"
"#define LOCAL __shared__\n"
"#define LOCAL_ARG\n"
"#define GLOBAL\n"
"#define RESTRICT __restrict__\n"
"#define LOCAL_ID threadIdx.x\n"
"#define LOCAL_SIZE blockDim.x\n"
"#define GLOBAL_ID (blockIdx.x*blockDim.x+threadIdx.x)\n"
"#define GLOBAL_SIZE (blockDim.x*gridDim.x)\n"
"#define GROUP_ID blockIdx.x\n"
"#define NUM_GROUPS gridDim.x\n"
"#define SYNC_THREADS __syncthreads();\n"
"#define MEM_FENCE __threadfence_block();\n"
"#define ATOMIC_ADD(dest, value) atomicAdd(dest, value)\n"
"\n"
"typedef long long mm_long;\n"
"typedef unsigned long long mm_ulong;\n"
"\n"
"#define SUPPORTS_64_BIT_ATOMICS 1\n"
"#define SUPPORTS_DOUBLE_PRECISION 1\n"
"";
const string CudaKernelSources::fft = "static __inline__ __device__ real2 multiplyComplex(real2 c1, real2 c2) {\n"
"    return make_real2(c1.x*c2.x-c1.y*c2.y, c1.x*c2.y+c1.y*c2.x);\n"
"}\n"
"\n"
"/**\n"
" * Load a value from the half-complex grid produces by a real-to-complex transform.\n"
" */\n"
"static __inline__ __device__ real2 loadComplexValue(const real2* __restrict__ in, int x, int y, int z) {\n"
"    const int inputZSize = ZSIZE/2+1;\n"
"    if (z < inputZSize)\n"
"        return in[x*YSIZE*inputZSize+y*inputZSize+z];\n"
"    int xp = (x == 0 ? 0 : XSIZE-x);\n"
"    int yp = (y == 0 ? 0 : YSIZE-y);\n"
"    real2 value = in[xp*YSIZE*inputZSize+yp*inputZSize+(ZSIZE-z)];\n"
"    return make_real2(value.x, -value.y);\n"
"}\n"
"\n"
"/**\n"
" * Perform a 1D FFT on each row along one axis.\n"
" */\n"
"\n"
"extern \"C\" __global__ void execFFT(const INPUT_TYPE* __restrict__ in, OUTPUT_TYPE* __restrict__ out) {\n"
"    __shared__ real2 w[ZSIZE];\n"
"    __shared__ real2 data0[BLOCKS_PER_GROUP*ZSIZE];\n"
"    __shared__ real2 data1[BLOCKS_PER_GROUP*ZSIZE];\n"
"    for (int i = threadIdx.x; i < ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(-(SIGN)*i*2*M_PI/ZSIZE), sin(-(SIGN)*i*2*M_PI/ZSIZE));\n"
"    __syncthreads();\n"
"    \n"
"    const int block = threadIdx.x/THREADS_PER_BLOCK;\n"
"    for (int baseIndex = blockIdx.x*BLOCKS_PER_GROUP; baseIndex < XSIZE*YSIZE; baseIndex += gridDim.x*BLOCKS_PER_GROUP) {\n"
"        int index = baseIndex+block;\n"
"        int x = index/YSIZE;\n"
"        int y = index-x*YSIZE;\n"
"#if OUTPUT_IS_PACKED\n"
"        if (x < XSIZE/2+1) {\n"
"#endif\n"
"        if (index < XSIZE*YSIZE)\n"
"            for (int i = threadIdx.x-block*THREADS_PER_BLOCK; i < ZSIZE; i += THREADS_PER_BLOCK)\n"
"    #if INPUT_IS_REAL\n"
"                data0[i+block*ZSIZE] = make_real2(in[x*(YSIZE*ZSIZE)+y*ZSIZE+i], 0);\n"
"    #elif INPUT_IS_PACKED\n"
"                data0[i+block*ZSIZE] = loadComplexValue(in, x, y, i);\n"
"    #else\n"
"                data0[i+block*ZSIZE] = in[x*(YSIZE*ZSIZE)+y*ZSIZE+i];\n"
"    #endif\n"
"#if OUTPUT_IS_PACKED\n"
"        }\n"
"#endif\n"
"        __syncthreads();\n"
"        COMPUTE_FFT\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::fftR2C = "/**\n"
" * Combine the two halves of a real grid into a complex grid that is half as large.\n"
" */\n"
"extern \"C\" __global__ void packForwardData(const real* __restrict__ in, real2* __restrict__ out) {\n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"#if PACKED_AXIS == 0\n"
"        real2 value = make_real2(in[2*x*YSIZE*ZSIZE+y*ZSIZE+z], in[(2*x+1)*YSIZE*ZSIZE+y*ZSIZE+z]);\n"
"#elif PACKED_AXIS == 1\n"
"        real2 value = make_real2(in[x*YSIZE*ZSIZE+2*y*ZSIZE+z], in[x*YSIZE*ZSIZE+(2*y+1)*ZSIZE+z]);\n"
"#else\n"
"        real2 value = make_real2(in[x*YSIZE*ZSIZE+y*ZSIZE+2*z], in[x*YSIZE*ZSIZE+y*ZSIZE+(2*z+1)]);\n"
"#endif\n"
"        out[index] = value;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Split the transformed data back into a full sized, symmetric grid.\n"
" */\n"
"extern \"C\" __global__ void unpackForwardData(const real2* __restrict__ in, real2* __restrict__ out) {\n"
"    // Compute the phase factors.\n"
"    \n"
"#if PACKED_AXIS == 0\n"
"    __shared__ real2 w[PACKED_XSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_XSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/XSIZE), cos(i*2*M_PI/XSIZE));\n"
"#elif PACKED_AXIS == 1\n"
"    __shared__ real2 w[PACKED_YSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_YSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/YSIZE), cos(i*2*M_PI/YSIZE));\n"
"#else\n"
"    __shared__ real2 w[PACKED_ZSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/ZSIZE), cos(i*2*M_PI/ZSIZE));\n"
"#endif\n"
"    __syncthreads();\n"
"\n"
"    // Transform the data.\n"
"    \n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    const int outputZSize = ZSIZE/2+1;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        int xp = (x == 0 ? 0 : PACKED_XSIZE-x);\n"
"        int yp = (y == 0 ? 0 : PACKED_YSIZE-y);\n"
"        int zp = (z == 0 ? 0 : PACKED_ZSIZE-z);\n"
"        real2 z1 = in[x*PACKED_YSIZE*PACKED_ZSIZE+y*PACKED_ZSIZE+z];\n"
"        real2 z2 = in[xp*PACKED_YSIZE*PACKED_ZSIZE+yp*PACKED_ZSIZE+zp];\n"
"#if PACKED_AXIS == 0\n"
"        real2 wfac = w[x];\n"
"#elif PACKED_AXIS == 1\n"
"        real2 wfac = w[y];\n"
"#else\n"
"        real2 wfac = w[z];\n"
"#endif\n"
"        real2 output = make_real2((z1.x+z2.x - wfac.x*(z1.x-z2.x) + wfac.y*(z1.y+z2.y))/2, (z1.y-z2.y - wfac.y*(z1.x-z2.x) - wfac.x*(z1.y+z2.y))/2);\n"
"        if (z < outputZSize)\n"
"            out[x*YSIZE*outputZSize+y*outputZSize+z] = output;\n"
"        xp = (x == 0 ? 0 : XSIZE-x);\n"
"        yp = (y == 0 ? 0 : YSIZE-y);\n"
"        zp = (z == 0 ? 0 : ZSIZE-z);\n"
"        if (zp < outputZSize) {\n"
"#if PACKED_AXIS == 0\n"
"            if (x == 0)\n"
"                out[PACKED_XSIZE*YSIZE*outputZSize+yp*outputZSize+zp] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#elif PACKED_AXIS == 1\n"
"            if (y == 0)\n"
"                out[xp*YSIZE*outputZSize+PACKED_YSIZE*outputZSize+zp] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#else\n"
"            if (z == 0)\n"
"                out[xp*YSIZE*outputZSize+yp*outputZSize+PACKED_ZSIZE] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#endif\n"
"            else\n"
"                out[xp*YSIZE*outputZSize+yp*outputZSize+zp] = make_real2(output.x, -output.y);\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Load a value from the half-complex grid produced by a real-to-complex transform.\n"
" */\n"
"static __inline__ __device__ real2 loadComplexValue(const real2* __restrict__ in, int x, int y, int z) {\n"
"    const int inputZSize = ZSIZE/2+1;\n"
"    if (z < inputZSize)\n"
"        return in[x*YSIZE*inputZSize+y*inputZSize+z];\n"
"    int xp = (x == 0 ? 0 : XSIZE-x);\n"
"    int yp = (y == 0 ? 0 : YSIZE-y);\n"
"    real2 value = in[xp*YSIZE*inputZSize+yp*inputZSize+(ZSIZE-z)];\n"
"    return make_real2(value.x, -value.y);\n"
"}\n"
"\n"
"/**\n"
" * Repack the symmetric complex grid into one half as large in preparation for doing an inverse complex-to-real transform.\n"
" */\n"
"extern \"C\" __global__ void packBackwardData(const real2* __restrict__ in, real2* __restrict__ out) {\n"
"    // Compute the phase factors.\n"
"    \n"
"#if PACKED_AXIS == 0\n"
"    __shared__ real2 w[PACKED_XSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_XSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/XSIZE), sin(i*2*M_PI/XSIZE));\n"
"#elif PACKED_AXIS == 1\n"
"    __shared__ real2 w[PACKED_YSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_YSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/YSIZE), sin(i*2*M_PI/YSIZE));\n"
"#else\n"
"    __shared__ real2 w[PACKED_ZSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/ZSIZE), sin(i*2*M_PI/ZSIZE));\n"
"#endif\n"
"    __syncthreads();\n"
"\n"
"    // Transform the data.\n"
"    \n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        int xp = (x == 0 ? 0 : PACKED_XSIZE-x);\n"
"        int yp = (y == 0 ? 0 : PACKED_YSIZE-y);\n"
"        int zp = (z == 0 ? 0 : PACKED_ZSIZE-z);\n"
"        real2 z1 = loadComplexValue(in, x, y, z);\n"
"#if PACKED_AXIS == 0\n"
"        real2 wfac = w[x];\n"
"        real2 z2 = loadComplexValue(in, PACKED_XSIZE-x, yp, zp);\n"
"#elif PACKED_AXIS == 1\n"
"        real2 wfac = w[y];\n"
"        real2 z2 = loadComplexValue(in, xp, PACKED_YSIZE-y, zp);\n"
"#else\n"
"        real2 wfac = w[z];\n"
"        real2 z2 = loadComplexValue(in, xp, yp, PACKED_ZSIZE-z);\n"
"#endif\n"
"        real2 even = make_real2((z1.x+z2.x)/2, (z1.y-z2.y)/2);\n"
"        real2 odd = make_real2((z1.x-z2.x)/2, (z1.y+z2.y)/2);\n"
"        odd = make_real2(odd.x*wfac.x-odd.y*wfac.y, odd.y*wfac.x+odd.x*wfac.y);\n"
"        out[x*PACKED_YSIZE*PACKED_ZSIZE+y*PACKED_ZSIZE+z] = make_real2(even.x-odd.y, even.y+odd.x);\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Split the data back into a full sized, real grid after an inverse transform.\n"
" */\n"
"extern \"C\" __global__ void unpackBackwardData(const real2* __restrict__ in, real* __restrict__ out) {\n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        real2 value = 2*in[index];\n"
"#if PACKED_AXIS == 0\n"
"        out[2*x*YSIZE*ZSIZE+y*ZSIZE+z] = value.x;\n"
"        out[(2*x+1)*YSIZE*ZSIZE+y*ZSIZE+z] = value.y;\n"
"#elif PACKED_AXIS == 1\n"
"        out[x*YSIZE*ZSIZE+2*y*ZSIZE+z] = value.x;\n"
"        out[x*YSIZE*ZSIZE+(2*y+1)*ZSIZE+z] = value.y;\n"
"#else\n"
"        out[x*YSIZE*ZSIZE+y*ZSIZE+2*z] = value.x;\n"
"        out[x*YSIZE*ZSIZE+y*ZSIZE+(2*z+1)] = value.y;\n"
"#endif\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::findInteractingBlocks = "#define GROUP_SIZE 256\n"
"#define BUFFER_SIZE 256\n"
"\n"
"/**\n"
" * Find a bounding box for the atoms in each block.\n"
" */\n"
"extern \"C\" __global__ void findBlockBounds(int numAtoms, real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        const real4* __restrict__ posq, real4* __restrict__ blockCenter, real4* __restrict__ blockBoundingBox, int* __restrict__ rebuildNeighborList,\n"
"        real2* __restrict__ sortedBlocks) {\n"
"    int index = blockIdx.x*blockDim.x+threadIdx.x;\n"
"    int base = index*TILE_SIZE;\n"
"    while (base < numAtoms) {\n"
"        real4 pos = posq[base];\n"
"#ifdef USE_PERIODIC\n"
"        APPLY_PERIODIC_TO_POS(pos)\n"
"#endif\n"
"        real4 minPos = pos;\n"
"        real4 maxPos = pos;\n"
"        int last = min(base+TILE_SIZE, numAtoms);\n"
"        for (int i = base+1; i < last; i++) {\n"
"            pos = posq[i];\n"
"#ifdef USE_PERIODIC\n"
"            real4 center = 0.5f*(maxPos+minPos);\n"
"            APPLY_PERIODIC_TO_POS_WITH_CENTER(pos, center)\n"
"#endif\n"
"            minPos = make_real4(min(minPos.x,pos.x), min(minPos.y,pos.y), min(minPos.z,pos.z), 0);\n"
"            maxPos = make_real4(max(maxPos.x,pos.x), max(maxPos.y,pos.y), max(maxPos.z,pos.z), 0);\n"
"        }\n"
"        real4 blockSize = 0.5f*(maxPos-minPos);\n"
"        real4 center = 0.5f*(maxPos+minPos);\n"
"        center.w = 0;\n"
"        for (int i = base; i < last; i++) {\n"
"            pos = posq[i];\n"
"            real4 delta = posq[i]-center;\n"
"#ifdef USE_PERIODIC\n"
"            APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"            center.w = max(center.w, delta.x*delta.x+delta.y*delta.y+delta.z*delta.z);\n"
"        }\n"
"        center.w = sqrt(center.w);\n"
"        blockBoundingBox[index] = blockSize;\n"
"        blockCenter[index] = center;\n"
"        sortedBlocks[index] = make_real2(blockSize.x+blockSize.y+blockSize.z, index);\n"
"        index += blockDim.x*gridDim.x;\n"
"        base = index*TILE_SIZE;\n"
"    }\n"
"    if (blockIdx.x == 0 && threadIdx.x == 0)\n"
"        rebuildNeighborList[0] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Sort the data about bounding boxes so it can be accessed more efficiently in the next kernel.\n"
" */\n"
"extern \"C\" __global__ void sortBoxData(const real2* __restrict__ sortedBlock, const real4* __restrict__ blockCenter,\n"
"        const real4* __restrict__ blockBoundingBox, real4* __restrict__ sortedBlockCenter,\n"
"        real4* __restrict__ sortedBlockBoundingBox, const real4* __restrict__ posq, const real4* __restrict__ oldPositions,\n"
"        unsigned int* __restrict__ interactionCount, int* __restrict__ rebuildNeighborList, bool forceRebuild) {\n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_BLOCKS; i += blockDim.x*gridDim.x) {\n"
"        int index = (int) sortedBlock[i].y;\n"
"        sortedBlockCenter[i] = blockCenter[index];\n"
"        sortedBlockBoundingBox[i] = blockBoundingBox[index];\n"
"    }\n"
"    \n"
"    // Also check whether any atom has moved enough so that we really need to rebuild the neighbor list.\n"
"\n"
"    bool rebuild = forceRebuild;\n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x) {\n"
"        real4 delta = oldPositions[i]-posq[i];\n"
"        if (delta.x*delta.x + delta.y*delta.y + delta.z*delta.z > 0.25f*PADDING*PADDING)\n"
"            rebuild = true;\n"
"    }\n"
"    if (rebuild) {\n"
"        rebuildNeighborList[0] = 1;\n"
"        interactionCount[0] = 0;\n"
"        interactionCount[1] = 0;\n"
"    }\n"
"}\n"
"\n"
"__device__ int saveSinglePairs(int x, int* atoms, int* flags, int length, unsigned int maxSinglePairs, unsigned int* singlePairCount, int2* singlePairs, int* sumBuffer, volatile int& pairStartIndex) {\n"
"    // Record interactions that should be computed as single pairs rather than in blocks.\n"
"    \n"
"    const int indexInWarp = threadIdx.x%32;\n"
"    int sum = 0;\n"
"    #pragma unroll 8 // (GROUP_SIZE / TILE_SIZE)\n"
"    for (int i = indexInWarp; i < length; i += 32) {\n"
"        int count = __popc(flags[i]);\n"
"        sum += (count <= MAX_BITS_FOR_PAIRS ? count : 0);\n"
"    }\n"
"    for (int i = 1; i < 32; i *= 2) {\n"
"        int n = __shfl_up_sync(0xffffffff, sum, i);\n"
"        if (indexInWarp >= i)\n"
"            sum += n;\n"
"    }\n"
"    if (indexInWarp == 31)\n"
"        pairStartIndex = atomicAdd(singlePairCount,(unsigned int) sum);\n"
"    __syncwarp();\n"
"    int prevSum = __shfl_up_sync(0xffffffff, sum, 1);\n"
"    int pairIndex = pairStartIndex + (indexInWarp > 0 ? prevSum : 0);\n"
"    for (int i = indexInWarp; i < length; i += 32) {\n"
"        int count = __popc(flags[i]);\n"
"        if (count <= MAX_BITS_FOR_PAIRS && pairIndex+count <= maxSinglePairs) {\n"
"            int f = flags[i];\n"
"            while (f != 0) {\n"
"                singlePairs[pairIndex] = make_int2(atoms[i], x*TILE_SIZE+__ffs(f)-1);\n"
"                f &= f-1;\n"
"                pairIndex++;\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    // Compact the remaining interactions.\n"
"    \n"
"    const int warpMask = (1<<indexInWarp)-1;\n"
"    int numCompacted = 0;\n"
"    for (int start = 0; start < length; start += 32) {\n"
"        int i = start+indexInWarp;\n"
"        int atom = atoms[i];\n"
"        int flag = flags[i];\n"
"        bool include = (i < length && __popc(flags[i]) > MAX_BITS_FOR_PAIRS);\n"
"        int includeFlags = BALLOT(include);\n"
"        if (include) {\n"
"            int index = numCompacted+__popc(includeFlags&warpMask);\n"
"            atoms[index] = atom;\n"
"            flags[index] = flag;\n"
"        }\n"
"        numCompacted += __popc(includeFlags);\n"
"    }\n"
"    return numCompacted;\n"
"}\n"
"\n"
"/**\n"
" * Compare the bounding boxes for each pair of atom blocks (comprised of 32 atoms each), forming a tile. If the two\n"
" * atom blocks are sufficiently far apart, mark them as non-interacting. There are two stages in the algorithm.\n"
" *\n"
" * STAGE 1:\n"
" *\n"
" * A coarse grained atom block against interacting atom block neighbour list is constructed. \n"
" *\n"
" * Each warp first loads in some block X of interest. Each thread within the warp then loads \n"
" * in a different atom block Y. If Y has exclusions with X, then Y is not processed.  If the bounding boxes \n"
" * of the two atom blocks are within the cutoff distance, then the two atom blocks are considered to be\n"
" * interacting and Y is added to the buffer for X.\n"
" *\n"
" * STAGE 2:\n"
" *\n"
" * A fine grained atom block against interacting atoms neighbour list is constructed.\n"
" *\n"
" * The warp loops over atom blocks Y that were found to (possibly) interact with atom block X.  Each thread\n"
" * in the warp loops over the 32 atoms in X and compares their positions to one particular atom from block Y.\n"
" * If it finds one closer than the cutoff distance, the atom is added to the list of atoms interacting with block X.\n"
" * This continues until the buffer fills up, at which point the results are written to global memory.\n"
" *\n"
" * [in] periodicBoxSize        - size of the rectangular periodic box\n"
" * [in] invPeriodicBoxSize     - inverse of the periodic box\n"
" * [in] blockCenter            - the center of each bounding box\n"
" * [in] blockBoundingBox       - bounding box of each atom block\n"
" * [out] interactionCount      - total number of tiles that have interactions\n"
" * [out] interactingTiles      - set of blocks that have interactions\n"
" * [out] interactingAtoms      - a list of atoms that interact with each atom block\n"
" * [in] posq                   - x,y,z coordinates of each atom and charge q\n"
" * [in] maxTiles               - maximum number of tiles to process, used for multi-GPUs\n"
" * [in] startBlockIndex        - first block to process, used for multi-GPUs,\n"
" * [in] numBlocks              - total number of atom blocks\n"
" * [in] sortedBlocks           - a sorted list of atom blocks based on volume\n"
" * [in] sortedBlockCenter      - sorted centers, duplicated for fast access to avoid indexing\n"
" * [in] sortedBlockBoundingBox - sorted bounding boxes, duplicated for fast access\n"
" * [in] exclusionIndices       - maps into exclusionRowIndices with the starting position for a given atom\n"
" * [in] exclusionRowIndices    - stores the a continuous list of exclusions\n"
" *           eg: block 0 is excluded from atom 3,5,6\n"
" *               block 1 is excluded from atom 3,4\n"
" *               block 2 is excluded from atom 1,3,5,6\n"
" *              exclusionIndices[0][3][5][8]\n"
" *           exclusionRowIndices[3][5][6][3][4][1][3][5][6]\n"
" *                         index 0  1  2  3  4  5  6  7  8 \n"
" * [out] oldPos                - stores the positions of the atoms in which this neighbourlist was built on\n"
" *                             - this is used to decide when to rebuild a neighbourlist\n"
" * [in] rebuildNeighbourList   - whether or not to execute this kernel\n"
" *\n"
" */\n"
"extern \"C\" __global__ __launch_bounds__(GROUP_SIZE,3) void findBlocksWithInteractions(real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        unsigned int* __restrict__ interactionCount, int* __restrict__ interactingTiles, unsigned int* __restrict__ interactingAtoms,\n"
"        int2* __restrict__ singlePairs, const real4* __restrict__ posq, unsigned int maxTiles, unsigned int maxSinglePairs,\n"
"        unsigned int startBlockIndex, unsigned int numBlocks, real2* __restrict__ sortedBlocks, const real4* __restrict__ sortedBlockCenter,\n"
"        const real4* __restrict__ sortedBlockBoundingBox, const unsigned int* __restrict__ exclusionIndices, const unsigned int* __restrict__ exclusionRowIndices,\n"
"        real4* __restrict__ oldPositions, const int* __restrict__ rebuildNeighborList) {\n"
"\n"
"    if (rebuildNeighborList[0] == 0)\n"
"        return; // The neighbor list doesn't need to be rebuilt.\n"
"\n"
"    const int indexInWarp = threadIdx.x%32;\n"
"    const int warpStart = threadIdx.x-indexInWarp;\n"
"    const int totalWarps = blockDim.x*gridDim.x/32;\n"
"    const int warpIndex = (blockIdx.x*blockDim.x+threadIdx.x)/32;\n"
"    const int warpMask = (1<<indexInWarp)-1;\n"
"    __shared__ int workgroupBuffer[BUFFER_SIZE*(GROUP_SIZE/32)];\n"
"    __shared__ int workgroupFlagsBuffer[BUFFER_SIZE*(GROUP_SIZE/32)];\n"
"    __shared__ int warpExclusions[MAX_EXCLUSIONS*(GROUP_SIZE/32)];\n"
"    __shared__ real4 posBuffer[GROUP_SIZE];\n"
"    __shared__ volatile int workgroupTileIndex[GROUP_SIZE/32];\n"
"    __shared__ int worksgroupPairStartIndex[GROUP_SIZE/32];\n"
"    int* sumBuffer = (int*) posBuffer; // Reuse the same buffer to save memory\n"
"    int* buffer = workgroupBuffer+BUFFER_SIZE*(warpStart/32);\n"
"    int* flagsBuffer = workgroupFlagsBuffer+BUFFER_SIZE*(warpStart/32);\n"
"    int* exclusionsForX = warpExclusions+MAX_EXCLUSIONS*(warpStart/32);\n"
"    volatile int& tileStartIndex = workgroupTileIndex[warpStart/32];\n"
"    volatile int& pairStartIndex = worksgroupPairStartIndex[warpStart/32];\n"
"\n"
"    // Loop over blocks.\n"
"    \n"
"    for (int block1 = startBlockIndex+warpIndex; block1 < startBlockIndex+numBlocks; block1 += totalWarps) {\n"
"        // Load data for this block.  Note that all threads in a warp are processing the same block.\n"
"        \n"
"        real2 sortedKey = sortedBlocks[block1];\n"
"        int x = (int) sortedKey.y;\n"
"        real4 blockCenterX = sortedBlockCenter[block1];\n"
"        real4 blockSizeX = sortedBlockBoundingBox[block1];\n"
"        int neighborsInBuffer = 0;\n"
"        real4 pos1 = posq[x*TILE_SIZE+indexInWarp];\n"
"#ifdef USE_PERIODIC\n"
"        const bool singlePeriodicCopy = (0.5f*periodicBoxSize.x-blockSizeX.x >= PADDED_CUTOFF &&\n"
"                                         0.5f*periodicBoxSize.y-blockSizeX.y >= PADDED_CUTOFF &&\n"
"                                         0.5f*periodicBoxSize.z-blockSizeX.z >= PADDED_CUTOFF);\n"
"        if (singlePeriodicCopy) {\n"
"            // The box is small enough that we can just translate all the atoms into a single periodic\n"
"            // box, then skip having to apply periodic boundary conditions later.\n"
"            \n"
"            APPLY_PERIODIC_TO_POS_WITH_CENTER(pos1, blockCenterX)\n"
"        }\n"
"#endif\n"
"        pos1.w = 0.5f * (pos1.x * pos1.x + pos1.y * pos1.y + pos1.z * pos1.z);\n"
"        posBuffer[threadIdx.x] = pos1;\n"
"\n"
"        // Load exclusion data for block x.\n"
"        \n"
"        const int exclusionStart = exclusionRowIndices[x];\n"
"        const int exclusionEnd = exclusionRowIndices[x+1];\n"
"        const int numExclusions = exclusionEnd-exclusionStart;\n"
"        #pragma unroll 4 // (MAX_EXCLUSIONS)\n"
"        for (int j = indexInWarp; j < numExclusions; j += 32)\n"
"            exclusionsForX[j] = exclusionIndices[exclusionStart+j];\n"
"        if (MAX_EXCLUSIONS > 32)\n"
"            __syncthreads();\n"
"        \n"
"        // Loop over atom blocks to search for neighbors.  The threads in a warp compare block1 against 32\n"
"        // other blocks in parallel.\n"
"\n"
"        for (int block2Base = block1+1; block2Base < NUM_BLOCKS; block2Base += 32) {\n"
"            int block2 = block2Base+indexInWarp;\n"
"            bool includeBlock2 = (block2 < NUM_BLOCKS);\n"
"            bool forceInclude = false;\n"
"            if (includeBlock2) {\n"
"                real4 blockCenterY = sortedBlockCenter[block2];\n"
"                real4 blockSizeY = sortedBlockBoundingBox[block2];\n"
"                real4 blockDelta = blockCenterX-blockCenterY;\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(blockDelta)\n"
"#endif\n"
"                includeBlock2 &= (blockDelta.x*blockDelta.x+blockDelta.y*blockDelta.y+blockDelta.z*blockDelta.z < (PADDED_CUTOFF+blockCenterX.w+blockCenterY.w)*(PADDED_CUTOFF+blockCenterX.w+blockCenterY.w));\n"
"                blockDelta.x = max(0.0f, fabs(blockDelta.x)-blockSizeX.x-blockSizeY.x);\n"
"                blockDelta.y = max(0.0f, fabs(blockDelta.y)-blockSizeX.y-blockSizeY.y);\n"
"                blockDelta.z = max(0.0f, fabs(blockDelta.z)-blockSizeX.z-blockSizeY.z);\n"
"                includeBlock2 &= (blockDelta.x*blockDelta.x+blockDelta.y*blockDelta.y+blockDelta.z*blockDelta.z < PADDED_CUTOFF_SQUARED);\n"
"#ifdef TRICLINIC\n"
"                // The calculation to find the nearest periodic copy is only guaranteed to work if the nearest copy is less than half a box width away.\n"
"                // If there's any possibility we might have missed it, do a detailed check.\n"
"\n"
"                if (periodicBoxSize.z/2-blockSizeX.z-blockSizeY.z < PADDED_CUTOFF || periodicBoxSize.y/2-blockSizeX.y-blockSizeY.y < PADDED_CUTOFF)\n"
"                    includeBlock2 = forceInclude = true;\n"
"#endif\n"
"                if (includeBlock2) {\n"
"                    int y = (int) sortedBlocks[block2].y;\n"
"                    #pragma unroll 4 // (MAX_EXCLUSIONS)\n"
"                    for (int k = 0; k < numExclusions; k++)\n"
"                        includeBlock2 &= (exclusionsForX[k] != y);\n"
"                }\n"
"            }\n"
"            \n"
"            // Loop over any blocks we identified as potentially containing neighbors.\n"
"            \n"
"            int includeBlockFlags = BALLOT(includeBlock2);\n"
"            int forceIncludeFlags = BALLOT(forceInclude);\n"
"            while (includeBlockFlags != 0) {\n"
"                int i = __ffs(includeBlockFlags)-1;\n"
"                includeBlockFlags &= includeBlockFlags-1;\n"
"                forceInclude = (forceIncludeFlags>>i) & 1;\n"
"                int y = (int) sortedBlocks[block2Base+i].y;\n"
"\n"
"                // Check each atom in block Y for interactions.\n"
"\n"
"                int atom2 = y*TILE_SIZE+indexInWarp;\n"
"                real4 pos2 = posq[atom2];\n"
"#ifdef USE_PERIODIC\n"
"                if (singlePeriodicCopy) {\n"
"                    APPLY_PERIODIC_TO_POS_WITH_CENTER(pos2, blockCenterX)\n"
"                }\n"
"#endif\n"
"                pos2.w = 0.5f * (pos2.x * pos2.x + pos2.y * pos2.y + pos2.z * pos2.z);\n"
"\n"
"                real4 blockCenterY = sortedBlockCenter[block2Base+i];\n"
"                real3 atomDelta = trimTo3(posBuffer[warpStart+indexInWarp])-trimTo3(blockCenterY);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(atomDelta)\n"
"#endif\n"
"                int atomFlags = BALLOT(forceInclude || atomDelta.x*atomDelta.x+atomDelta.y*atomDelta.y+atomDelta.z*atomDelta.z < (PADDED_CUTOFF+blockCenterY.w)*(PADDED_CUTOFF+blockCenterY.w));\n"
"                int interacts = 0;\n"
"                if (atom2 < NUM_ATOMS && atomFlags != 0) {\n"
"#ifdef USE_PERIODIC\n"
"                    if (!singlePeriodicCopy) {\n"
"                        int first = __ffs(atomFlags)-1;\n"
"                        int last = 32-__clz(atomFlags);\n"
"                        for (int j = first; j < last; j++) {\n"
"                            real3 delta = trimTo3(pos2)-trimTo3(posBuffer[warpStart+j]);\n"
"                            APPLY_PERIODIC_TO_DELTA(delta)\n"
"                            interacts |= (delta.x*delta.x+delta.y*delta.y+delta.z*delta.z < PADDED_CUTOFF_SQUARED ? 1<<j : 0);\n"
"                        }\n"
"                    }\n"
"                    else {\n"
"#endif\n"
"                        #pragma unroll\n"
"                        for (int j = 0; j < 32; j++) {\n"
"                            real4 posj = posBuffer[warpStart+j];\n"
"                            real halfDist2 = posj.w + pos2.w - posj.x*pos2.x - posj.y*pos2.y - posj.z*pos2.z;\n"
"                            interacts |= (halfDist2 < 0.5f * PADDED_CUTOFF_SQUARED ? 1<<j : 0);\n"
"                        }\n"
"#ifdef USE_PERIODIC\n"
"                    }\n"
"#endif\n"
"                }\n"
"                \n"
"                // Add any interacting atoms to the buffer.\n"
"                \n"
"                int includeAtomFlags = BALLOT(interacts);\n"
"                if (interacts) {\n"
"                    int index = neighborsInBuffer+__popc(includeAtomFlags&warpMask);\n"
"                    buffer[index] = atom2;\n"
"                    flagsBuffer[index] = interacts;\n"
"                }\n"
"                neighborsInBuffer += __popc(includeAtomFlags);\n"
"                if (neighborsInBuffer > BUFFER_SIZE-TILE_SIZE) {\n"
"                    // Store the new tiles to memory.\n"
"                    \n"
"#if MAX_BITS_FOR_PAIRS > 0\n"
"                    neighborsInBuffer = saveSinglePairs(x, buffer, flagsBuffer, neighborsInBuffer, maxSinglePairs, &interactionCount[1], singlePairs, sumBuffer+warpStart, pairStartIndex);\n"
"#endif\n"
"                    int tilesToStore = neighborsInBuffer/TILE_SIZE;\n"
"                    if (tilesToStore > 0) {\n"
"                        if (indexInWarp == 0)\n"
"                            tileStartIndex = atomicAdd(&interactionCount[0], tilesToStore);\n"
"                        int newTileStartIndex = tileStartIndex;\n"
"                        if (newTileStartIndex+tilesToStore <= maxTiles) {\n"
"                            if (indexInWarp < tilesToStore)\n"
"                                interactingTiles[newTileStartIndex+indexInWarp] = x;\n"
"                            #pragma unroll 8 // (GROUP_SIZE / TILE_SIZE)\n"
"                            for (int j = 0; j < tilesToStore; j++)\n"
"                                interactingAtoms[(newTileStartIndex+j)*TILE_SIZE+indexInWarp] = buffer[indexInWarp+j*TILE_SIZE];\n"
"                        }\n"
"                        if (indexInWarp+TILE_SIZE*tilesToStore < BUFFER_SIZE)\n"
"                            buffer[indexInWarp] = buffer[indexInWarp+TILE_SIZE*tilesToStore];\n"
"                        neighborsInBuffer -= TILE_SIZE*tilesToStore;\n"
"                    }\n"
"                }\n"
"            }\n"
"        }\n"
"        \n"
"        // If we have a partially filled buffer,  store it to memory.\n"
"        \n"
"#if MAX_BITS_FOR_PAIRS > 0\n"
"        if (neighborsInBuffer > 32)\n"
"            neighborsInBuffer = saveSinglePairs(x, buffer, flagsBuffer, neighborsInBuffer, maxSinglePairs, &interactionCount[1], singlePairs, sumBuffer+warpStart, pairStartIndex);\n"
"#endif\n"
"        if (neighborsInBuffer > 0) {\n"
"            int tilesToStore = (neighborsInBuffer+TILE_SIZE-1)/TILE_SIZE;\n"
"            if (indexInWarp == 0)\n"
"                tileStartIndex = atomicAdd(&interactionCount[0], tilesToStore);\n"
"            int newTileStartIndex = tileStartIndex;\n"
"            if (newTileStartIndex+tilesToStore <= maxTiles) {\n"
"                if (indexInWarp < tilesToStore)\n"
"                    interactingTiles[newTileStartIndex+indexInWarp] = x;\n"
"                #pragma unroll 8 // (GROUP_SIZE / TILE_SIZE)\n"
"                for (int j = 0; j < tilesToStore; j++)\n"
"                    interactingAtoms[(newTileStartIndex+j)*TILE_SIZE+indexInWarp] = (indexInWarp+j*TILE_SIZE < neighborsInBuffer ? buffer[indexInWarp+j*TILE_SIZE] : NUM_ATOMS);\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    // Record the positions the neighbor list is based on.\n"
"    \n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x)\n"
"        oldPositions[i] = posq[i];\n"
"}\n"
"";
const string CudaKernelSources::nonbonded = "#define WARPS_PER_GROUP (THREAD_BLOCK_SIZE/TILE_SIZE)\n"
"\n"
"#ifndef ENABLE_SHUFFLE\n"
"typedef struct {\n"
"    real x, y, z;\n"
"    real q;\n"
"    real fx, fy, fz;\n"
"    ATOM_PARAMETER_DATA\n"
"#ifndef PARAMETER_SIZE_IS_EVEN\n"
"    real padding;\n"
"#endif\n"
"} AtomData;\n"
"#endif\n"
"\n"
"#ifdef ENABLE_SHUFFLE\n"
"//support for 64 bit shuffles\n"
"static __inline__ __device__ float real_shfl(float var, int srcLane) {\n"
"    return SHFL(var, srcLane);\n"
"}\n"
"\n"
"static __inline__ __device__ float real_shfl(int var, int srcLane) {\n"
"    return SHFL(var, srcLane);\n"
"}\n"
"\n"
"static __inline__ __device__ double real_shfl(double var, int srcLane) {\n"
"    int hi, lo;\n"
"    asm volatile(\"mov.b64 { %0, %1 }, %2;\" : \"=r\"(lo), \"=r\"(hi) : \"d\"(var));\n"
"    hi = SHFL(hi, srcLane);\n"
"    lo = SHFL(lo, srcLane);\n"
"    return __hiloint2double( hi, lo );\n"
"}\n"
"\n"
"static __inline__ __device__ long long real_shfl(long long var, int srcLane) {\n"
"    int hi, lo;\n"
"    asm volatile(\"mov.b64 { %0, %1 }, %2;\" : \"=r\"(lo), \"=r\"(hi) : \"l\"(var));\n"
"    hi = SHFL(hi, srcLane);\n"
"    lo = SHFL(lo, srcLane);\n"
"    // unforunately there isn't an __nv_hiloint2long(hi,lo) intrinsic cast\n"
"    int2 fuse; fuse.x = lo; fuse.y = hi;\n"
"    return *reinterpret_cast<long long*>(&fuse);\n"
"}\n"
"#endif\n"
"\n"
"/**\n"
" * Compute nonbonded interactions. The kernel is separated into two parts,\n"
" * tiles with exclusions and tiles without exclusions. It relies heavily on \n"
" * implicit warp-level synchronization. A tile is defined by two atom blocks \n"
" * each of warpsize. Each warp computes a range of tiles.\n"
" * \n"
" * Tiles with exclusions compute the entire set of interactions across\n"
" * atom blocks, equal to warpsize*warpsize. In order to avoid access conflicts \n"
" * the forces are computed and accumulated diagonally in the manner shown below\n"
" * where, suppose\n"
" *\n"
" * [a-h] comprise atom block 1, [i-p] comprise atom block 2\n"
" *\n"
" * 1 denotes the first set of calculations within the warp\n"
" * 2 denotes the second set of calculations within the warp\n"
" * ... etc.\n"
" * \n"
" *        threads\n"
" *     0 1 2 3 4 5 6 7\n"
" *         atom1 \n"
" * L    a b c d e f g h \n"
" * o  i 1 2 3 4 5 6 7 8\n"
" * c  j 8 1 2 3 4 5 6 7\n"
" * a  k 7 8 1 2 3 4 5 6\n"
" * l  l 6 7 8 1 2 3 4 5\n"
" * D  m 5 6 7 8 1 2 3 4 \n"
" * a  n 4 5 6 7 8 1 2 3\n"
" * t  o 3 4 5 6 7 8 1 2\n"
" * a  p 2 3 4 5 6 7 8 1\n"
" *\n"
" * Tiles without exclusions read off directly from the neighbourlist interactingAtoms\n"
" * and follows the same force accumulation method. If more there are more interactingTiles\n"
" * than the size of the neighbourlist initially allocated, the neighbourlist is rebuilt\n"
" * and the full tileset is computed. This should happen on the first step, and very rarely \n"
" * afterwards.\n"
" *\n"
" * On CUDA devices that support the shuffle intrinsic, on diagonal exclusion tiles use\n"
" * __shfl to broadcast. For all other types of tiles __shfl is used to pass around the \n"
" * forces, positions, and parameters when computing the forces. \n"
" *\n"
" * [out]forceBuffers    - forces on each atom to eventually be accumulated\n"
" * [out]energyBuffer    - energyBuffer to eventually be accumulated\n"
" * [in]posq             - x,y,z,charge \n"
" * [in]exclusions       - 1024-bit flags denoting atom-atom exclusions for each tile\n"
" * [in]exclusionTiles   - x,y denotes the indices of tiles that have an exclusion\n"
" * [in]startTileIndex   - index into first tile to be processed\n"
" * [in]numTileIndices   - number of tiles this context is responsible for processing\n"
" * [in]int tiles        - the atom block for each tile\n"
" * [in]interactionCount - total number of tiles that have an interaction\n"
" * [in]maxTiles         - stores the size of the neighbourlist in case it needs \n"
" *                      - to be expanded\n"
" * [in]periodicBoxSize  - size of the Periodic Box, last dimension (w) not used\n"
" * [in]invPeriodicBox   - inverse of the periodicBoxSize, pre-computed for speed\n"
" * [in]blockCenter      - the center of each block in euclidean coordinates\n"
" * [in]blockSize        - size of the each block, radiating from the center\n"
" *                      - x is half the distance of total length\n"
" *                      - y is half the distance of total width\n"
" *                      - z is half the distance of total height\n"
" *                      - w is not used\n"
" * [in]interactingAtoms - a list of interactions within a given tile     \n"
" *\n"
" */\n"
"extern \"C\" __global__ void computeNonbonded(\n"
"        unsigned long long* __restrict__ forceBuffers, mixed* __restrict__ energyBuffer, const real4* __restrict__ posq, const tileflags* __restrict__ exclusions,\n"
"        const int2* __restrict__ exclusionTiles, unsigned int startTileIndex, unsigned long long numTileIndices\n"
"#ifdef USE_CUTOFF\n"
"        , const int* __restrict__ tiles, const unsigned int* __restrict__ interactionCount, real4 periodicBoxSize, real4 invPeriodicBoxSize, \n"
"        real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ, unsigned int maxTiles, const real4* __restrict__ blockCenter,\n"
"        const real4* __restrict__ blockSize, const unsigned int* __restrict__ interactingAtoms, unsigned int maxSinglePairs,\n"
"        const int2* __restrict__ singlePairs\n"
"#endif\n"
"        PARAMETER_ARGUMENTS) {\n"
"    const unsigned int totalWarps = (blockDim.x*gridDim.x)/TILE_SIZE;\n"
"    const unsigned int warp = (blockIdx.x*blockDim.x+threadIdx.x)/TILE_SIZE; // global warpIndex\n"
"    const unsigned int tgx = threadIdx.x & (TILE_SIZE-1); // index within the warp\n"
"    const unsigned int tbx = threadIdx.x - tgx;           // block warpIndex\n"
"    mixed energy = 0;\n"
"    INIT_DERIVATIVES\n"
"    // used shared memory if the device cannot shuffle\n"
"#ifndef ENABLE_SHUFFLE\n"
"    __shared__ AtomData localData[THREAD_BLOCK_SIZE];\n"
"#endif\n"
"\n"
"    // First loop: process tiles that contain exclusions.\n"
"\n"
"    const unsigned int firstExclusionTile = FIRST_EXCLUSION_TILE+warp*(LAST_EXCLUSION_TILE-FIRST_EXCLUSION_TILE)/totalWarps;\n"
"    const unsigned int lastExclusionTile = FIRST_EXCLUSION_TILE+(warp+1)*(LAST_EXCLUSION_TILE-FIRST_EXCLUSION_TILE)/totalWarps;\n"
"    for (int pos = firstExclusionTile; pos < lastExclusionTile; pos++) {\n"
"        const int2 tileIndices = exclusionTiles[pos];\n"
"        const unsigned int x = tileIndices.x;\n"
"        const unsigned int y = tileIndices.y;\n"
"        real3 force = make_real3(0);\n"
"        unsigned int atom1 = x*TILE_SIZE + tgx;\n"
"        real4 posq1 = posq[atom1];\n"
"        LOAD_ATOM1_PARAMETERS\n"
"#ifdef USE_EXCLUSIONS\n"
"        tileflags excl = exclusions[pos*TILE_SIZE+tgx];\n"
"#endif\n"
"        const bool hasExclusions = true;\n"
"        if (x == y) {\n"
"            // This tile is on the diagonal.\n"
"#ifdef ENABLE_SHUFFLE\n"
"            real4 shflPosq = posq1;\n"
"#else\n"
"            localData[threadIdx.x].x = posq1.x;\n"
"            localData[threadIdx.x].y = posq1.y;\n"
"            localData[threadIdx.x].z = posq1.z;\n"
"            localData[threadIdx.x].q = posq1.w;\n"
"            LOAD_LOCAL_PARAMETERS_FROM_1\n"
"#endif\n"
"\n"
"            // we do not need to fetch parameters from global since this is a symmetric tile\n"
"            // instead we can broadcast the values using shuffle\n"
"            for (unsigned int j = 0; j < TILE_SIZE; j++) {\n"
"                int atom2 = tbx+j;\n"
"                real4 posq2;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                BROADCAST_WARP_DATA\n"
"#else   \n"
"                posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                real invR = RSQRT(r2);\n"
"                real r = r2*invR;\n"
"                LOAD_ATOM2_PARAMETERS\n"
"                atom2 = y*TILE_SIZE+j;\n"
"#ifdef USE_SYMMETRIC\n"
"                real dEdR = 0.0f;\n"
"#else\n"
"                real3 dEdR1 = make_real3(0);\n"
"                real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS || !(excl & 0x1));\n"
"#endif\n"
"                real tempEnergy = 0.0f;\n"
"                const real interactionScale = 0.5f;\n"
"                COMPUTE_INTERACTION\n"
"                energy += 0.5f*tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                force.x -= delta.x*dEdR;\n"
"                force.y -= delta.y*dEdR;\n"
"                force.z -= delta.z*dEdR;\n"
"#else\n"
"                force.x -= dEdR1.x;\n"
"                force.y -= dEdR1.y;\n"
"                force.z -= dEdR1.z;\n"
"#endif\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                excl >>= 1;\n"
"#endif\n"
"            }\n"
"        }\n"
"        else {\n"
"            // This is an off-diagonal tile.\n"
"            unsigned int j = y*TILE_SIZE + tgx;\n"
"            real4 shflPosq = posq[j];\n"
"#ifdef ENABLE_SHUFFLE\n"
"            real3 shflForce;\n"
"            shflForce.x = 0.0f;\n"
"            shflForce.y = 0.0f;\n"
"            shflForce.z = 0.0f;\n"
"#else\n"
"            localData[threadIdx.x].x = shflPosq.x;\n"
"            localData[threadIdx.x].y = shflPosq.y;\n"
"            localData[threadIdx.x].z = shflPosq.z;\n"
"            localData[threadIdx.x].q = shflPosq.w;\n"
"            localData[threadIdx.x].fx = 0.0f;\n"
"            localData[threadIdx.x].fy = 0.0f;\n"
"            localData[threadIdx.x].fz = 0.0f;\n"
"#endif\n"
"            DECLARE_LOCAL_PARAMETERS\n"
"            LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"#ifdef USE_EXCLUSIONS\n"
"            excl = (excl >> tgx) | (excl << (TILE_SIZE - tgx));\n"
"#endif\n"
"            unsigned int tj = tgx;\n"
"            for (j = 0; j < TILE_SIZE; j++) {\n"
"                int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                real4 posq2 = shflPosq;\n"
"#else\n"
"                real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                real invR = RSQRT(r2);\n"
"                real r = r2*invR;\n"
"                LOAD_ATOM2_PARAMETERS\n"
"                atom2 = y*TILE_SIZE+tj;\n"
"#ifdef USE_SYMMETRIC\n"
"                real dEdR = 0.0f;\n"
"#else\n"
"                real3 dEdR1 = make_real3(0);\n"
"                real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS || !(excl & 0x1));\n"
"#endif\n"
"                real tempEnergy = 0.0f;\n"
"                const real interactionScale = 1.0f;\n"
"                COMPUTE_INTERACTION\n"
"                energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                delta *= dEdR;\n"
"                force.x -= delta.x;\n"
"                force.y -= delta.y;\n"
"                force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflForce.x += delta.x;\n"
"                shflForce.y += delta.y;\n"
"                shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                localData[tbx+tj].fx += delta.x;\n"
"                localData[tbx+tj].fy += delta.y;\n"
"                localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                force.x -= dEdR1.x;\n"
"                force.y -= dEdR1.y;\n"
"                force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflForce.x += dEdR2.x;\n"
"                shflForce.y += dEdR2.y;\n"
"                shflForce.z += dEdR2.z;\n"
"#else\n"
"                localData[tbx+tj].fx += dEdR2.x;\n"
"                localData[tbx+tj].fy += dEdR2.y;\n"
"                localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                SHUFFLE_WARP_DATA\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                excl >>= 1;\n"
"#endif\n"
"                // cycles the indices\n"
"                // 0 1 2 3 4 5 6 7 -> 1 2 3 4 5 6 7 0\n"
"                tj = (tj + 1) & (TILE_SIZE - 1);\n"
"            }\n"
"            const unsigned int offset = y*TILE_SIZE + tgx;\n"
"            // write results for off diagonal tiles\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef ENABLE_SHUFFLE\n"
"            atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (shflForce.x*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.y*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.z*0x100000000)));\n"
"#else\n"
"            atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fx*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fy*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fz*0x100000000)));\n"
"#endif\n"
"#endif\n"
"        }\n"
"        // Write results for on and off diagonal tiles\n"
"#ifdef INCLUDE_FORCES\n"
"        const unsigned int offset = x*TILE_SIZE + tgx;\n"
"        atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (force.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.z*0x100000000)));\n"
"#endif\n"
"    }\n"
"\n"
"    // Second loop: tiles without exclusions, either from the neighbor list (with cutoff) or just enumerating all\n"
"    // of them (no cutoff).\n"
"\n"
"#ifdef USE_CUTOFF\n"
"    const unsigned int numTiles = interactionCount[0];\n"
"    if (numTiles > maxTiles)\n"
"        return; // There wasn't enough memory for the neighbor list.\n"
"    int pos = (int) (warp*(long long)numTiles/totalWarps);\n"
"    int end = (int) ((warp+1)*(long long)numTiles/totalWarps);\n"
"#else\n"
"    int pos = (int) (startTileIndex+warp*numTileIndices/totalWarps);\n"
"    int end = (int) (startTileIndex+(warp+1)*numTileIndices/totalWarps);\n"
"#endif\n"
"    int skipBase = 0;\n"
"    int currentSkipIndex = tbx;\n"
"    // atomIndices can probably be shuffled as well\n"
"    // but it probably wouldn't make things any faster\n"
"    __shared__ int atomIndices[THREAD_BLOCK_SIZE];\n"
"    __shared__ volatile int skipTiles[THREAD_BLOCK_SIZE];\n"
"    skipTiles[threadIdx.x] = -1;\n"
"    \n"
"    while (pos < end) {\n"
"        const bool hasExclusions = false;\n"
"        real3 force = make_real3(0);\n"
"        bool includeTile = true;\n"
"\n"
"        // Extract the coordinates of this tile.\n"
"        int x, y;\n"
"        bool singlePeriodicCopy = false;\n"
"#ifdef USE_CUTOFF\n"
"        x = tiles[pos];\n"
"        real4 blockSizeX = blockSize[x];\n"
"        singlePeriodicCopy = (0.5f*periodicBoxSize.x-blockSizeX.x >= MAX_CUTOFF &&\n"
"                              0.5f*periodicBoxSize.y-blockSizeX.y >= MAX_CUTOFF &&\n"
"                              0.5f*periodicBoxSize.z-blockSizeX.z >= MAX_CUTOFF);\n"
"#else\n"
"        y = (int) floor(NUM_BLOCKS+0.5f-SQRT((NUM_BLOCKS+0.5f)*(NUM_BLOCKS+0.5f)-2*pos));\n"
"        x = (pos-y*NUM_BLOCKS+y*(y+1)/2);\n"
"        if (x < y || x >= NUM_BLOCKS) { // Occasionally happens due to roundoff error.\n"
"            y += (x < y ? -1 : 1);\n"
"            x = (pos-y*NUM_BLOCKS+y*(y+1)/2);\n"
"        }\n"
"\n"
"        // Skip over tiles that have exclusions, since they were already processed.\n"
"\n"
"        while (skipTiles[tbx+TILE_SIZE-1] < pos) {\n"
"            if (skipBase+tgx < NUM_TILES_WITH_EXCLUSIONS) {\n"
"                int2 tile = exclusionTiles[skipBase+tgx];\n"
"                skipTiles[threadIdx.x] = tile.x + tile.y*NUM_BLOCKS - tile.y*(tile.y+1)/2;\n"
"            }\n"
"            else\n"
"                skipTiles[threadIdx.x] = end;\n"
"            skipBase += TILE_SIZE;            \n"
"            currentSkipIndex = tbx;\n"
"        }\n"
"        while (skipTiles[currentSkipIndex] < pos)\n"
"            currentSkipIndex++;\n"
"        includeTile = (skipTiles[currentSkipIndex] != pos);\n"
"#endif\n"
"        if (includeTile) {\n"
"            unsigned int atom1 = x*TILE_SIZE + tgx;\n"
"            // Load atom data for this tile.\n"
"            real4 posq1 = posq[atom1];\n"
"            LOAD_ATOM1_PARAMETERS\n"
"            //const unsigned int localAtomIndex = threadIdx.x;\n"
"#ifdef USE_CUTOFF\n"
"            unsigned int j = interactingAtoms[pos*TILE_SIZE+tgx];\n"
"#else\n"
"            unsigned int j = y*TILE_SIZE + tgx;\n"
"#endif\n"
"            atomIndices[threadIdx.x] = j;\n"
"#ifdef ENABLE_SHUFFLE\n"
"            DECLARE_LOCAL_PARAMETERS\n"
"            real4 shflPosq;\n"
"            real3 shflForce;\n"
"            shflForce.x = 0.0f;\n"
"            shflForce.y = 0.0f;\n"
"            shflForce.z = 0.0f;\n"
"#endif\n"
"            if (j < PADDED_NUM_ATOMS) {\n"
"                // Load position of atom j from from global memory\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflPosq = posq[j];\n"
"#else\n"
"                localData[threadIdx.x].x = posq[j].x;\n"
"                localData[threadIdx.x].y = posq[j].y;\n"
"                localData[threadIdx.x].z = posq[j].z;\n"
"                localData[threadIdx.x].q = posq[j].w;\n"
"                localData[threadIdx.x].fx = 0.0f;\n"
"                localData[threadIdx.x].fy = 0.0f;\n"
"                localData[threadIdx.x].fz = 0.0f;\n"
"#endif                \n"
"                LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"            }\n"
"            else {\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflPosq = make_real4(0, 0, 0, 0);\n"
"#else\n"
"                localData[threadIdx.x].x = 0;\n"
"                localData[threadIdx.x].y = 0;\n"
"                localData[threadIdx.x].z = 0;\n"
"#endif\n"
"                CLEAR_LOCAL_PARAMETERS\n"
"            }\n"
"#ifdef USE_PERIODIC\n"
"            if (singlePeriodicCopy) {\n"
"                // The box is small enough that we can just translate all the atoms into a single periodic\n"
"                // box, then skip having to apply periodic boundary conditions later.\n"
"                real4 blockCenterX = blockCenter[x];\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(posq1, blockCenterX)\n"
"#ifdef ENABLE_SHUFFLE\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(shflPosq, blockCenterX)\n"
"#else\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(localData[threadIdx.x], blockCenterX)\n"
"#endif\n"
"                unsigned int tj = tgx;\n"
"                for (j = 0; j < TILE_SIZE; j++) {\n"
"                    int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    real4 posq2 = shflPosq; \n"
"#else\n"
"                    real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                    real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"                    real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                    real invR = RSQRT(r2);\n"
"                    real r = r2*invR;\n"
"                    LOAD_ATOM2_PARAMETERS\n"
"                    atom2 = atomIndices[tbx+tj];\n"
"#ifdef USE_SYMMETRIC\n"
"                    real dEdR = 0.0f;\n"
"#else\n"
"                    real3 dEdR1 = make_real3(0);\n"
"                    real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                    bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS);\n"
"#endif\n"
"                    real tempEnergy = 0.0f;\n"
"                    const real interactionScale = 1.0f;\n"
"                    COMPUTE_INTERACTION\n"
"                    energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                    delta *= dEdR;\n"
"                    force.x -= delta.x;\n"
"                    force.y -= delta.y;\n"
"                    force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += delta.x;\n"
"                    shflForce.y += delta.y;\n"
"                    shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                    localData[tbx+tj].fx += delta.x;\n"
"                    localData[tbx+tj].fy += delta.y;\n"
"                    localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                    force.x -= dEdR1.x;\n"
"                    force.y -= dEdR1.y;\n"
"                    force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += dEdR2.x;\n"
"                    shflForce.y += dEdR2.y;\n"
"                    shflForce.z += dEdR2.z;\n"
"#else\n"
"                    localData[tbx+tj].fx += dEdR2.x;\n"
"                    localData[tbx+tj].fy += dEdR2.y;\n"
"                    localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    SHUFFLE_WARP_DATA\n"
"#endif\n"
"                    tj = (tj + 1) & (TILE_SIZE - 1);\n"
"                }\n"
"            }\n"
"            else\n"
"#endif\n"
"            {\n"
"                // We need to apply periodic boundary conditions separately for each interaction.\n"
"                unsigned int tj = tgx;\n"
"                for (j = 0; j < TILE_SIZE; j++) {\n"
"                    int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    real4 posq2 = shflPosq;\n"
"#else\n"
"                    real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                    real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                    APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                    real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                    real invR = RSQRT(r2);\n"
"                    real r = r2*invR;\n"
"                    LOAD_ATOM2_PARAMETERS\n"
"                    atom2 = atomIndices[tbx+tj];\n"
"#ifdef USE_SYMMETRIC\n"
"                    real dEdR = 0.0f;\n"
"#else\n"
"                    real3 dEdR1 = make_real3(0);\n"
"                    real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                    bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS);\n"
"#endif\n"
"                    real tempEnergy = 0.0f;\n"
"                    const real interactionScale = 1.0f;\n"
"                    COMPUTE_INTERACTION\n"
"                    energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                    delta *= dEdR;\n"
"                    force.x -= delta.x;\n"
"                    force.y -= delta.y;\n"
"                    force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += delta.x;\n"
"                    shflForce.y += delta.y;\n"
"                    shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                    localData[tbx+tj].fx += delta.x;\n"
"                    localData[tbx+tj].fy += delta.y;\n"
"                    localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                    force.x -= dEdR1.x;\n"
"                    force.y -= dEdR1.y;\n"
"                    force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += dEdR2.x;\n"
"                    shflForce.y += dEdR2.y;\n"
"                    shflForce.z += dEdR2.z;\n"
"#else\n"
"                    localData[tbx+tj].fx += dEdR2.x;\n"
"                    localData[tbx+tj].fy += dEdR2.y;\n"
"                    localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    SHUFFLE_WARP_DATA\n"
"#endif\n"
"                    tj = (tj + 1) & (TILE_SIZE - 1);\n"
"                }\n"
"            }\n"
"\n"
"            // Write results.\n"
"#ifdef INCLUDE_FORCES\n"
"            atomicAdd(&forceBuffers[atom1], static_cast<unsigned long long>((long long) (force.x*0x100000000)));\n"
"            atomicAdd(&forceBuffers[atom1+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.y*0x100000000)));\n"
"            atomicAdd(&forceBuffers[atom1+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.z*0x100000000)));\n"
"#ifdef USE_CUTOFF\n"
"            unsigned int atom2 = atomIndices[threadIdx.x];\n"
"#else\n"
"            unsigned int atom2 = y*TILE_SIZE + tgx;\n"
"#endif\n"
"            if (atom2 < PADDED_NUM_ATOMS) {\n"
"#ifdef ENABLE_SHUFFLE\n"
"                atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (shflForce.x*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.y*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.z*0x100000000)));\n"
"#else\n"
"                atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fx*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fy*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fz*0x100000000)));\n"
"#endif\n"
"            }\n"
"#endif\n"
"        }\n"
"        pos++;\n"
"    }\n"
"    \n"
"    // Third loop: single pairs that aren't part of a tile.\n"
"    \n"
"#if USE_CUTOFF\n"
"    const unsigned int numPairs = interactionCount[1];\n"
"    if (numPairs > maxSinglePairs)\n"
"        return; // There wasn't enough memory for the neighbor list.\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numPairs; i += blockDim.x*gridDim.x) {\n"
"        int2 pair = singlePairs[i];\n"
"        int atom1 = pair.x;\n"
"        int atom2 = pair.y;\n"
"        real4 posq1 = posq[atom1];\n"
"        real4 posq2 = posq[atom2];\n"
"        LOAD_ATOM1_PARAMETERS\n"
"        int j = atom2;\n"
"        atom2 = threadIdx.x;\n"
"        DECLARE_LOCAL_PARAMETERS\n"
"        LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"        LOAD_ATOM2_PARAMETERS\n"
"        atom2 = pair.y;\n"
"        real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"        APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"        real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"        real invR = RSQRT(r2);\n"
"        real r = r2*invR;\n"
"#ifdef USE_SYMMETRIC\n"
"        real dEdR = 0.0f;\n"
"#else\n"
"        real3 dEdR1 = make_real3(0);\n"
"        real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"        bool hasExclusions = false;\n"
"        bool isExcluded = false;\n"
"        real tempEnergy = 0.0f;\n"
"        const real interactionScale = 1.0f;\n"
"        COMPUTE_INTERACTION\n"
"        energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"        real3 dEdR1 = delta*dEdR;\n"
"        real3 dEdR2 = -dEdR1;\n"
"#endif\n"
"        atomicAdd(&forceBuffers[atom1], static_cast<unsigned long long>((long long) (-dEdR1.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom1+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR1.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom1+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR1.z*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (-dEdR2.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR2.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR2.z*0x100000000)));\n"
"#endif\n"
"    }\n"
"#endif\n"
"#ifdef INCLUDE_ENERGY\n"
"    energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] += energy;\n"
"#endif\n"
"    SAVE_DERIVATIVES\n"
"}";
const string CudaKernelSources::parallel = "/**\n"
" * Sum the forces computed by different contexts.\n"
" */\n"
"\n"
"extern \"C\" __global__ void sumForces(long long* __restrict__ force, long long* __restrict__ buffer, int bufferSize, int numBuffers) {\n"
"    int totalSize = bufferSize*numBuffers;\n"
"    for (int index = blockDim.x*blockIdx.x+threadIdx.x; index < bufferSize; index += blockDim.x*gridDim.x) {\n"
"        long long sum = force[index];\n"
"        for (int i = index; i < totalSize; i += bufferSize)\n"
"            sum += buffer[i];\n"
"        force[index] = sum;\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::sort = "__device__ KEY_TYPE getValue(DATA_TYPE value) {\n"
"    return SORT_KEY;\n"
"}\n"
"\n"
"extern \"C\" {\n"
"\n"
"/**\n"
" * Sort a list that is short enough to entirely fit in local memory.  This is executed as\n"
" * a single thread block.\n"
" */\n"
"__global__ void sortShortList(DATA_TYPE* __restrict__ data, unsigned int length) {\n"
"    // Load the data into local memory.\n"
"    \n"
"    extern __shared__ DATA_TYPE dataBuffer[];\n"
"    for (int index = threadIdx.x; index < length; index += blockDim.x)\n"
"        dataBuffer[index] = data[index];\n"
"    __syncthreads();\n"
"\n"
"    // Perform a bitonic sort in local memory.\n"
"\n"
"    for (unsigned int k = 2; k < 2*length; k *= 2) {\n"
"        for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"            for (unsigned int i = threadIdx.x; i < length; i += blockDim.x) {\n"
"                int ixj = i^j;\n"
"                if (ixj > i && ixj < length) {\n"
"                    DATA_TYPE value1 = dataBuffer[i];\n"
"                    DATA_TYPE value2 = dataBuffer[ixj];\n"
"                    bool ascending = ((i&k) == 0);\n"
"                    for (unsigned int mask = k*2; mask < 2*length; mask *= 2)\n"
"                        ascending = ((i&mask) == 0 ? !ascending : ascending);\n"
"                    KEY_TYPE lowKey  = (ascending ? getValue(value1) : getValue(value2));\n"
"                    KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                    if (lowKey > highKey) {\n"
"                        dataBuffer[i] = value2;\n"
"                        dataBuffer[ixj] = value1;\n"
"                    }\n"
"                }\n"
"            }\n"
"            __syncthreads();\n"
"        }\n"
"    }\n"
"\n"
"    // Write the data back to global memory.\n"
"\n"
"    for (int index = threadIdx.x; index < length; index += blockDim.x)\n"
"        data[index] = dataBuffer[index];\n"
"}\n"
"\n"
"/**\n"
" * An alternate kernel for sorting short lists.  In this version every thread does a full\n"
" * scan through the data to select the destination for one element.  This involves more\n"
" * work, but also parallelizes much better.\n"
" */\n"
"__global__ void sortShortList2(const DATA_TYPE* __restrict__ dataIn, DATA_TYPE* __restrict__ dataOut, unsigned int length) {\n"
"    __shared__ DATA_TYPE dataBuffer[64];\n"
"    int globalId = blockDim.x*blockIdx.x+threadIdx.x;\n"
"    DATA_TYPE value = dataIn[globalId < length ? globalId : 0];\n"
"    KEY_TYPE key = getValue(value);\n"
"    int count = 0;\n"
"    for (int blockStart = 0; blockStart < length; blockStart += blockDim.x) {\n"
"        int numInBlock = min(blockDim.x, length-blockStart);\n"
"        __syncthreads();\n"
"        if (threadIdx.x < numInBlock)\n"
"            dataBuffer[threadIdx.x] = dataIn[blockStart+threadIdx.x];\n"
"        __syncthreads();\n"
"        for (int i = 0; i < numInBlock; i++) {\n"
"            KEY_TYPE otherKey = getValue(dataBuffer[i]);\n"
"            if (otherKey < key || (otherKey == key && blockStart+i < globalId))\n"
"                count++;\n"
"        }\n"
"    }\n"
"    if (globalId < length)\n"
"        dataOut[count] = value;\n"
"}\n"
"\n"
"/**\n"
" * Calculate the minimum and maximum value in the array to be sorted.  This kernel\n"
" * is executed as a single work group.\n"
" */\n"
"__global__ void computeRange(const DATA_TYPE* __restrict__ data, unsigned int length, KEY_TYPE* __restrict__ range,\n"
"        unsigned int numBuckets, unsigned int* __restrict__ bucketOffset) {\n"
"#if UNIFORM\n"
"    extern __shared__ KEY_TYPE minBuffer[];\n"
"    KEY_TYPE* maxBuffer = minBuffer+blockDim.x;\n"
"    KEY_TYPE minimum = MAX_KEY;\n"
"    KEY_TYPE maximum = MIN_KEY;\n"
"\n"
"    // Each thread calculates the range of a subset of values.\n"
"\n"
"    for (unsigned int index = threadIdx.x; index < length; index += blockDim.x) {\n"
"        KEY_TYPE value = getValue(data[index]);\n"
"        minimum = min(minimum, value);\n"
"        maximum = max(maximum, value);\n"
"    }\n"
"\n"
"    // Now reduce them.\n"
"\n"
"    minBuffer[threadIdx.x] = minimum;\n"
"    maxBuffer[threadIdx.x] = maximum;\n"
"    __syncthreads();\n"
"    for (unsigned int step = 1; step < blockDim.x; step *= 2) {\n"
"        if (threadIdx.x+step < blockDim.x && threadIdx.x%(2*step) == 0) {\n"
"            minBuffer[threadIdx.x] = min(minBuffer[threadIdx.x], minBuffer[threadIdx.x+step]);\n"
"            maxBuffer[threadIdx.x] = max(maxBuffer[threadIdx.x], maxBuffer[threadIdx.x+step]);\n"
"        }\n"
"        __syncthreads();\n"
"    }\n"
"    minimum = minBuffer[0];\n"
"    maximum = maxBuffer[0];\n"
"    if (threadIdx.x == 0) {\n"
"        range[0] = minimum;\n"
"        range[1] = maximum;\n"
"    }\n"
"#endif\n"
"\n"
"    // Clear the bucket counters in preparation for the next kernel.\n"
"\n"
"    for (unsigned int index = threadIdx.x; index < numBuckets; index += blockDim.x)\n"
"        bucketOffset[index] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Assign elements to buckets.  This version is optimized for uniformly distributed data.\n"
" */\n"
"__global__ void assignElementsToBuckets(const DATA_TYPE* __restrict__ data, unsigned int length, unsigned int numBuckets, const KEY_TYPE* __restrict__ range,\n"
"        unsigned int* __restrict__ bucketOffset, unsigned int* __restrict__ bucketOfElement, unsigned int* __restrict__ offsetInBucket) {\n"
"    float minValue = (float) (range[0]);\n"
"    float maxValue = (float) (range[1]);\n"
"    float bucketWidth = (maxValue-minValue)/numBuckets;\n"
"    for (unsigned int index = blockDim.x*blockIdx.x+threadIdx.x; index < length; index += blockDim.x*gridDim.x) {\n"
"        float key = (float) getValue(data[index]);\n"
"        unsigned int bucketIndex = min((unsigned int) ((key-minValue)/bucketWidth), numBuckets-1);\n"
"        offsetInBucket[index] = atomicAdd(&bucketOffset[bucketIndex], 1);\n"
"        bucketOfElement[index] = bucketIndex;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Assign elements to buckets.  This version is optimized for non-uniformly distributed data.\n"
" */\n"
"__global__ void assignElementsToBuckets2(const DATA_TYPE* __restrict__ data, unsigned int length, unsigned int numBuckets, const KEY_TYPE* __restrict__ range,\n"
"        unsigned int* __restrict__ bucketOffset, unsigned int* __restrict__ bucketOfElement, unsigned int* __restrict__ offsetInBucket) {\n"
"    // Load 64 datapoints and sort them to get an estimate of the data distribution.\n"
"\n"
"    __shared__ KEY_TYPE elements[64];\n"
"    if (threadIdx.x < 64) {\n"
"        int index = (int) (threadIdx.x*length/64.0);\n"
"        elements[threadIdx.x] = getValue(data[index]);\n"
"    }\n"
"    __syncthreads();\n"
"    for (unsigned int k = 2; k <= 64; k *= 2) {\n"
"        for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"            if (threadIdx.x < 64) {\n"
"                int ixj = threadIdx.x^j;\n"
"                if (ixj > threadIdx.x) {\n"
"                    KEY_TYPE value1 = elements[threadIdx.x];\n"
"                    KEY_TYPE value2 = elements[ixj];\n"
"                    bool ascending = (threadIdx.x&k) == 0;\n"
"                    KEY_TYPE lowKey = (ascending ? value1 : value2);\n"
"                    KEY_TYPE highKey = (ascending ? value2 : value1);\n"
"                    if (lowKey > highKey) {\n"
"                        elements[threadIdx.x] = value2;\n"
"                        elements[ixj] = value1;\n"
"                    }\n"
"                }\n"
"            }\n"
"            __syncthreads();\n"
"        }\n"
"    }\n"
"\n"
"    // Create a function composed of linear segments mapping data values to bucket indices.\n"
"\n"
"    __shared__ float segmentLowerBound[9];\n"
"    __shared__ float segmentBaseIndex[9];\n"
"    __shared__ float segmentIndexScale[9];\n"
"    if (threadIdx.x == 0) {\n"
"        segmentLowerBound[0] = elements[0]-0.2f*(elements[5]-elements[0]);\n"
"        segmentLowerBound[1] = elements[5];\n"
"        segmentLowerBound[2] = elements[10];\n"
"        segmentLowerBound[3] = elements[20];\n"
"        segmentLowerBound[4] = elements[30];\n"
"        segmentLowerBound[5] = elements[40];\n"
"        segmentLowerBound[6] = elements[50];\n"
"        segmentLowerBound[7] = elements[60];\n"
"        segmentLowerBound[8] = elements[63]+0.2f*(elements[63]-elements[58]);\n"
"        segmentBaseIndex[0] = numBuckets/16;\n"
"        segmentBaseIndex[1] = 3*numBuckets/16;\n"
"        segmentBaseIndex[2] = 5*numBuckets/16;\n"
"        segmentBaseIndex[3] = 7*numBuckets/16;\n"
"        segmentBaseIndex[4] = 9*numBuckets/16;\n"
"        segmentBaseIndex[5] = 11*numBuckets/16;\n"
"        segmentBaseIndex[6] = 13*numBuckets/16;\n"
"        segmentBaseIndex[7] = 15*numBuckets/16;\n"
"        segmentBaseIndex[8] = numBuckets;\n"
"        for (int i = 0; i < 8; i++)\n"
"            if (segmentLowerBound[i+1] == segmentLowerBound[i])\n"
"                segmentIndexScale[i] = 0;\n"
"            else\n"
"                segmentIndexScale[i] = (segmentBaseIndex[i+1]-segmentBaseIndex[i])/(segmentLowerBound[i+1]-segmentLowerBound[i]);\n"
"    }\n"
"    __syncthreads();\n"
"\n"
"    // Assign elements to buckets.\n"
"\n"
"    for (unsigned int index = blockDim.x*blockIdx.x+threadIdx.x; index < length; index += blockDim.x*gridDim.x) {\n"
"        float key = (float) getValue(data[index]);\n"
"        int segment;\n"
"        for (segment = 0; segment < 7 && key > segmentLowerBound[segment+1]; segment++)\n"
"            ;\n"
"        unsigned int bucketIndex = segmentBaseIndex[segment]+(key-segmentLowerBound[segment])*segmentIndexScale[segment];\n"
"        bucketIndex = min(max(0, bucketIndex), numBuckets-1);\n"
"        offsetInBucket[index] = atomicAdd(&bucketOffset[bucketIndex], 1);\n"
"        bucketOfElement[index] = bucketIndex;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Sum the bucket sizes to compute the start position of each bucket.  This kernel\n"
" * is executed as a single work group.\n"
" */\n"
"__global__ void computeBucketPositions(unsigned int numBuckets, unsigned int* __restrict__ bucketOffset) {\n"
"    extern __shared__ unsigned int posBuffer[];\n"
"    unsigned int globalOffset = 0;\n"
"    for (unsigned int startBucket = 0; startBucket < numBuckets; startBucket += blockDim.x) {\n"
"        // Load the bucket sizes into local memory.\n"
"\n"
"        unsigned int globalIndex = startBucket+threadIdx.x;\n"
"        __syncthreads();\n"
"        posBuffer[threadIdx.x] = (globalIndex < numBuckets ? bucketOffset[globalIndex] : 0);\n"
"        __syncthreads();\n"
"\n"
"        // Perform a parallel prefix sum.\n"
"\n"
"        for (unsigned int step = 1; step < blockDim.x; step *= 2) {\n"
"            unsigned int add = (threadIdx.x >= step ? posBuffer[threadIdx.x-step] : 0);\n"
"            __syncthreads();\n"
"            posBuffer[threadIdx.x] += add;\n"
"            __syncthreads();\n"
"        }\n"
"\n"
"        // Write the results back to global memory.\n"
"\n"
"        if (globalIndex < numBuckets)\n"
"            bucketOffset[globalIndex] = posBuffer[threadIdx.x]+globalOffset;\n"
"        globalOffset += posBuffer[blockDim.x-1];\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Copy the input data into the buckets for sorting.\n"
" */\n"
"__global__ void copyDataToBuckets(const DATA_TYPE* __restrict__ data, DATA_TYPE* __restrict__ buckets, unsigned int length, const unsigned int* __restrict__ bucketOffset, const unsigned int* __restrict__ bucketOfElement, const unsigned int* __restrict__ offsetInBucket) {\n"
"    for (unsigned int index = blockDim.x*blockIdx.x+threadIdx.x; index < length; index += blockDim.x*gridDim.x) {\n"
"        DATA_TYPE element = data[index];\n"
"        unsigned int bucketIndex = bucketOfElement[index];\n"
"        unsigned int offset = (bucketIndex == 0 ? 0 : bucketOffset[bucketIndex-1]);\n"
"        buckets[offset+offsetInBucket[index]] = element;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Sort the data in each bucket.\n"
" */\n"
"__global__ void sortBuckets(DATA_TYPE* __restrict__ data, const DATA_TYPE* __restrict__ buckets, unsigned int numBuckets, const unsigned int* __restrict__ bucketOffset) {\n"
"    extern __shared__ DATA_TYPE dataBuffer[];\n"
"    for (unsigned int index = blockIdx.x; index < numBuckets; index += gridDim.x) {\n"
"        unsigned int startIndex = (index == 0 ? 0 : bucketOffset[index-1]);\n"
"        unsigned int endIndex = bucketOffset[index];\n"
"        unsigned int length = endIndex-startIndex;\n"
"        if (length <= blockDim.x) {\n"
"            // Load the data into local memory.\n"
"\n"
"            if (threadIdx.x < length)\n"
"                dataBuffer[threadIdx.x] = buckets[startIndex+threadIdx.x];\n"
"            else\n"
"                dataBuffer[threadIdx.x] = MAX_VALUE;\n"
"            __syncthreads();\n"
"\n"
"            // Perform a bitonic sort in local memory.\n"
"\n"
"            for (unsigned int k = 2; k <= blockDim.x; k *= 2) {\n"
"                for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"                    int ixj = threadIdx.x^j;\n"
"                    if (ixj > threadIdx.x) {\n"
"                        DATA_TYPE value1 = dataBuffer[threadIdx.x];\n"
"                        DATA_TYPE value2 = dataBuffer[ixj];\n"
"                        bool ascending = (threadIdx.x&k) == 0;\n"
"                        KEY_TYPE lowKey = (ascending ? getValue(value1) : getValue(value2));\n"
"                        KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                        if (lowKey > highKey) {\n"
"                            dataBuffer[threadIdx.x] = value2;\n"
"                            dataBuffer[ixj] = value1;\n"
"                        }\n"
"                    }\n"
"                    __syncthreads();\n"
"                }\n"
"            }\n"
"\n"
"            // Write the data to the sorted array.\n"
"\n"
"            if (threadIdx.x < length)\n"
"                data[startIndex+threadIdx.x] = dataBuffer[threadIdx.x];\n"
"        }\n"
"        else {\n"
"            // Copy the bucket data over to the output array.\n"
"\n"
"            for (unsigned int i = threadIdx.x; i < length; i += blockDim.x)\n"
"                data[startIndex+i] = buckets[startIndex+i];\n"
"            __threadfence_block();\n"
"            __syncthreads();\n"
"\n"
"            // Perform a bitonic sort in global memory.\n"
"\n"
"            for (unsigned int k = 2; k < 2*length; k *= 2) {\n"
"                for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"                    for (unsigned int i = threadIdx.x; i < length; i += blockDim.x) {\n"
"                        int ixj = i^j;\n"
"                        if (ixj > i && ixj < length) {\n"
"                            DATA_TYPE value1 = data[startIndex+i];\n"
"                            DATA_TYPE value2 = data[startIndex+ixj];\n"
"                            bool ascending = ((i&k) == 0);\n"
"                            for (unsigned int mask = k*2; mask < 2*length; mask *= 2)\n"
"                                ascending = ((i&mask) == 0 ? !ascending : ascending);\n"
"                            KEY_TYPE lowKey  = (ascending ? getValue(value1) : getValue(value2));\n"
"                            KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                            if (lowKey > highKey) {\n"
"                                data[startIndex+i] = value2;\n"
"                                data[startIndex+ixj] = value1;\n"
"                            }\n"
"                        }\n"
"                    }\n"
"                    __threadfence_block();\n"
"                    __syncthreads();\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"}";
const string CudaKernelSources::utilities = "extern \"C\" {\n"
"\n"
"/**\n"
" * This is called by the various functions below to clear a buffer.\n"
" */\n"
"__device__ void clearSingleBuffer(int* __restrict__ buffer, int size) {\n"
"    int index = blockDim.x*blockIdx.x+threadIdx.x;\n"
"    int4* buffer4 = (int4*) buffer;\n"
"    int sizeDiv4 = size/4;\n"
"    while (index < sizeDiv4) {\n"
"        buffer4[index] = make_int4(0);\n"
"        index += blockDim.x*gridDim.x;\n"
"    }\n"
"    if (blockDim.x*blockIdx.x+threadIdx.x == 0)\n"
"        for (int i = sizeDiv4*4; i < size; i++)\n"
"            buffer[i] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Fill a buffer with 0.\n"
" */\n"
"__global__ void clearBuffer(int* __restrict__ buffer, int size) {\n"
"    clearSingleBuffer(buffer, size);\n"
"}\n"
"\n"
"/**\n"
" * Fill two buffers with 0.\n"
" */\n"
"__global__ void clearTwoBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"}\n"
"\n"
"/**\n"
" * Fill three buffers with 0.\n"
" */\n"
"__global__ void clearThreeBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"}\n"
"\n"
"/**\n"
" * Fill four buffers with 0.\n"
" */\n"
"__global__ void clearFourBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"}\n"
"\n"
"/**\n"
" * Fill five buffers with 0.\n"
" */\n"
"__global__ void clearFiveBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4, int* __restrict__ buffer5, int size5) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"    clearSingleBuffer(buffer5, size5);\n"
"}\n"
"\n"
"/**\n"
" * Fill six buffers with 0.\n"
" */\n"
"__global__ void clearSixBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4, int* __restrict__ buffer5, int size5, int* __restrict__ buffer6, int size6) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"    clearSingleBuffer(buffer5, size5);\n"
"    clearSingleBuffer(buffer6, size6);\n"
"}\n"
"\n"
"/**\n"
" * Sum the energy buffer.\n"
" */\n"
"__global__ void reduceEnergy(const mixed* __restrict__ energyBuffer, mixed* __restrict__ result, int bufferSize, int workGroupSize) {\n"
"    extern __shared__ mixed tempBuffer[];\n"
"    const unsigned int thread = threadIdx.x;\n"
"    mixed sum = 0;\n"
"    for (unsigned int index = thread; index < bufferSize; index += blockDim.x)\n"
"        sum += energyBuffer[index];\n"
"    tempBuffer[thread] = sum;\n"
"    for (int i = 1; i < workGroupSize; i *= 2) {\n"
"        __syncthreads();\n"
"        if (thread%(i*2) == 0 && thread+i < workGroupSize)\n"
"            tempBuffer[thread] += tempBuffer[thread+i];\n"
"    }\n"
"    if (thread == 0)\n"
"        *result = tempBuffer[0];\n"
"}\n"
"\n"
"/**\n"
" * Record the atomic charges into the posq array.\n"
" */\n"
"__global__ void setCharges(real* __restrict__ charges, real4* __restrict__ posq, int* __restrict__ atomOrder, int numAtoms) {\n"
"    for (int i = blockDim.x*blockIdx.x+threadIdx.x; i < numAtoms; i += blockDim.x*gridDim.x)\n"
"        posq[i].w = charges[atomOrder[i]];\n"
"}\n"
"}\n"
"";
const string CudaKernelSources::vectorOps = "/**\n"
" * This file defines vector operations to simplify code elsewhere.\n"
" */\n"
"\n"
"// Versions of make_x() that take a single value and set all components to that.\n"
"\n"
"inline __device__ int2 make_int2(int a) {\n"
"    return make_int2(a, a);\n"
"}\n"
"\n"
"inline __device__ int3 make_int3(int a) {\n"
"    return make_int3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ int4 make_int4(int a) {\n"
"    return make_int4(a, a, a, a);\n"
"}\n"
"\n"
"inline __device__ float2 make_float2(float a) {\n"
"    return make_float2(a, a);\n"
"}\n"
"\n"
"inline __device__ float3 make_float3(float a) {\n"
"    return make_float3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ float4 make_float4(float a) {\n"
"    return make_float4(a, a, a, a);\n"
"}\n"
"\n"
"inline __device__ double2 make_double2(double a) {\n"
"    return make_double2(a, a);\n"
"}\n"
"\n"
"inline __device__ double3 make_double3(double a) {\n"
"    return make_double3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ double4 make_double4(double a) {\n"
"    return make_double4(a, a, a, a);\n"
"}\n"
"\n"
"// Negate a vector.\n"
"\n"
"inline __device__ int2 operator-(int2 a) {\n"
"    return make_int2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator-(int3 a) {\n"
"    return make_int3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator-(int4 a) {\n"
"    return make_int4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator-(float2 a) {\n"
"    return make_float2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator-(float3 a) {\n"
"    return make_float3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator-(float4 a) {\n"
"    return make_float4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator-(double2 a) {\n"
"    return make_double2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator-(double3 a) {\n"
"    return make_double3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator-(double4 a) {\n"
"    return make_double4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"// Add two vectors.\n"
"\n"
"inline __device__ int2 operator+(int2 a, int2 b) {\n"
"    return make_int2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator+(int3 a, int3 b) {\n"
"    return make_int3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator+(int4 a, int4 b) {\n"
"    return make_int4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator+(float2 a, float2 b) {\n"
"    return make_float2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator+(float3 a, float3 b) {\n"
"    return make_float3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator+(float4 a, float4 b) {\n"
"    return make_float4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator+(double2 a, double2 b) {\n"
"    return make_double2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator+(double3 a, double3 b) {\n"
"    return make_double3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator+(double4 a, double4 b) {\n"
"    return make_double4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"// Subtract two vectors.\n"
"\n"
"inline __device__ int2 operator-(int2 a, int2 b) {\n"
"    return make_int2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator-(int3 a, int3 b) {\n"
"    return make_int3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator-(int4 a, int4 b) {\n"
"    return make_int4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator-(float2 a, float2 b) {\n"
"    return make_float2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator-(float3 a, float3 b) {\n"
"    return make_float3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator-(float4 a, float4 b) {\n"
"    return make_float4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator-(double2 a, double2 b) {\n"
"    return make_double2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator-(double3 a, double3 b) {\n"
"    return make_double3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator-(double4 a, double4 b) {\n"
"    return make_double4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"// Multiply two vectors.\n"
"\n"
"inline __device__ int2 operator*(int2 a, int2 b) {\n"
"    return make_int2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int3 a, int3 b) {\n"
"    return make_int3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int4 a, int4 b) {\n"
"    return make_int4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float2 a, float2 b) {\n"
"    return make_float2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float3 a, float3 b) {\n"
"    return make_float3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float4 a, float4 b) {\n"
"    return make_float4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double2 a, double2 b) {\n"
"    return make_double2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double3 a, double3 b) {\n"
"    return make_double3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double4 a, double4 b) {\n"
"    return make_double4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"// Divide two vectors.\n"
"\n"
"inline __device__ int2 operator/(int2 a, int2 b) {\n"
"    return make_int2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator/(int3 a, int3 b) {\n"
"    return make_int3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator/(int4 a, int4 b) {\n"
"    return make_int4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator/(float2 a, float2 b) {\n"
"    return make_float2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator/(float3 a, float3 b) {\n"
"    return make_float3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator/(float4 a, float4 b) {\n"
"    return make_float4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator/(double2 a, double2 b) {\n"
"    return make_double2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator/(double3 a, double3 b) {\n"
"    return make_double3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator/(double4 a, double4 b) {\n"
"    return make_double4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"// += operator\n"
"\n"
"inline __device__ void operator+=(int2& a, int2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(int3& a, int3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(int4& a, int4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float2& a, float2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float3& a, float3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float4& a, float4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double2& a, double2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double3& a, double3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double4& a, double4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"// -= operator\n"
"\n"
"inline __device__ void operator-=(int2& a, int2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(int3& a, int3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(int4& a, int4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float2& a, float2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float3& a, float3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float4& a, float4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double2& a, double2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double3& a, double3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double4& a, double4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"// *= operator\n"
"\n"
"inline __device__ void operator*=(int2& a, int2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int3& a, int3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int4& a, int4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float2& a, float2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float3& a, float3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float4& a, float4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double2& a, double2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double3& a, double3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double4& a, double4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"// /= operator\n"
"\n"
"inline __device__ void operator/=(int2& a, int2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(int3& a, int3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(int4& a, int4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float2& a, float2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float3& a, float3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float4& a, float4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double2& a, double2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double3& a, double3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double4& a, double4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"// Multiply a vector by a constant.\n"
"\n"
"inline __device__ int2 operator*(int2 a, int b) {\n"
"    return make_int2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int3 a, int b) {\n"
"    return make_int3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int4 a, int b) {\n"
"    return make_int4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ int2 operator*(int a, int2 b) {\n"
"    return make_int2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int a, int3 b) {\n"
"    return make_int3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int a, int4 b) {\n"
"    return make_int4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float2 a, float b) {\n"
"    return make_float2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float3 a, float b) {\n"
"    return make_float3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float4 a, float b) {\n"
"    return make_float4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float a, float2 b) {\n"
"    return make_float2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float a, float3 b) {\n"
"    return make_float3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float a, float4 b) {\n"
"    return make_float4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double2 a, double b) {\n"
"    return make_double2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double3 a, double b) {\n"
"    return make_double3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double4 a, double b) {\n"
"    return make_double4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double a, double2 b) {\n"
"    return make_double2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double a, double3 b) {\n"
"    return make_double3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double a, double4 b) {\n"
"    return make_double4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"// Divide a vector by a constant.\n"
"\n"
"inline __device__ int2 operator/(int2 a, int b) {\n"
"    return make_int2(a.x/b, a.y/b);\n"
"}\n"
"\n"
"inline __device__ int3 operator/(int3 a, int b) {\n"
"    return make_int3(a.x/b, a.y/b, a.z/b);\n"
"}\n"
"\n"
"inline __device__ int4 operator/(int4 a, int b) {\n"
"    return make_int4(a.x/b, a.y/b, a.z/b, a.w/b);\n"
"}\n"
"\n"
"inline __device__ float2 operator/(float2 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ float3 operator/(float3 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ float4 operator/(float4 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double2 operator/(double2 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double3 operator/(double3 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double4 operator/(double4 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"// *= operator (multiply vector by constant)\n"
"\n"
"inline __device__ void operator*=(int2& a, int b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int3& a, int b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int4& a, int b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float2& a, float b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float3& a, float b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float4& a, float b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double2& a, double b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double3& a, double b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double4& a, double b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"// Dot product\n"
"\n"
"inline __device__ float dot(float3 a, float3 b) {\n"
"    return a.x*b.x+a.y*b.y+a.z*b.z;\n"
"}\n"
"\n"
"inline __device__ double dot(double3 a, double3 b) {\n"
"    return a.x*b.x+a.y*b.y+a.z*b.z;\n"
"}\n"
"\n"
"// Cross product\n"
"\n"
"inline __device__ float3 cross(float3 a, float3 b) {\n"
"    return make_float3(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x);\n"
"}\n"
"\n"
"inline __device__ float4 cross(float4 a, float4 b) {\n"
"    return make_float4(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x, 0.0f);\n"
"}\n"
"\n"
"inline __device__ double3 cross(double3 a, double3 b) {\n"
"    return make_double3(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x);\n"
"}\n"
"\n"
"inline __device__ double4 cross(double4 a, double4 b) {\n"
"    return make_double4(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x, 0.0);\n"
"}\n"
"\n"
"// Normalize a vector\n"
"\n"
"inline __device__ float2 normalize(float2 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y);\n"
"}\n"
"\n"
"inline __device__ float3 normalize(float3 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y+a.z*a.z);\n"
"}\n"
"\n"
"inline __device__ float4 normalize(float4 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y+a.z*a.z+a.w*a.w);\n"
"}\n"
"\n"
"inline __device__ double2 normalize(double2 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y);\n"
"}\n"
"\n"
"inline __device__ double3 normalize(double3 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y+a.z*a.z);\n"
"}\n"
"\n"
"inline __device__ double4 normalize(double4 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y+a.z*a.z+a.w*a.w);\n"
"}\n"
"\n"
"// Strip off the fourth component of a vector.\n"
"\n"
"inline __device__ short3 trimTo3(short4 v) {\n"
"    return make_short3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ int3 trimTo3(int4 v) {\n"
"    return make_int3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ float3 trimTo3(float4 v) {\n"
"    return make_float3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ double3 trimTo3(double4 v) {\n"
"    return make_double3(v.x, v.y, v.z);\n"
"}\n"
"";
